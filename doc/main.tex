
\documentclass[12pt]{article}
% \usepackage[utf8]{inputenc}
\documentclass{article}
\usepackage{graphicx}
\graphicspath{ {images/} }

\usepackage[backend=bibtex,
%style=numeric,
style=alphabetic,
%style=reading,
sorting=ynt
]{biblatex}
\addbibresource{sample}

\title{Final Project Report}
\author{Xiao Liu, Wenxiang Zheng}

\begin{document}

\maketitle
\section{Abstract}
This paper is the final report of our project for Sentiment Analysis on movie reviews. In the introduction section, we review our proposal and literature reviews, and we also presents the layout of this final report. We provide some basic definitions and concepts that are related to such research topic in this section as well. In addition, the paper  provides a description of the dataset that will be tested in our research project. In the Baseline Research Methods section, we address some baseline methods for Aspect-based Sentiment analysis and ranking prediction in our project, and we provide some advantages and disadvantages for each method along with relavant examples. In the Experiment section, we present the testing results for each of the methods that has been tested in the final experiment. Finally, this paper concludes with a discussion about some of the problems that we faced during final experiment and the future work.
\section{Introduction}
In the first few weeks of this semester, we had been very interested in Sentiment Analysis. After our discussions and literature reviews, we decided to focus on Sentiment Analysis on movie reviews for our term project. 

	In our research proposal, we proposed the importance of Sentiment Analysis on movie reviews to both movie producers and customers, and we discussed how an automatic Sentiment Analysis application can help the movie review system. We also described the dataset that will be examined for this data mining project, and presented some candidate research questions for our term project. 

After discussing our research questions and the direction of this research with professor,  we have spotted the research question that we will work for throughout the semester for our term project. In the literature review, we present our research question and discuss some potential Sentiment Analysis methods from previous researches that can be implemented for our term project. In general, there are two major Sentiment Analysis methods: Sentiment Classification and Aspect-based Sentiment Analysis. After further discussion with the professor about our research topic and research method, we decided to work on Aspect-based Sentiment Analysis on movie reviews. 

In the Project Midterm Report, we addressed the well-defined research question, and demonstrated the baseline methods for this project in Sentiment Analysis on movie reviews. The base line methods are divided into three categories: Review Ranking Predictions, Aspect Extraction, and Aspect-Based Sentiment Analysis. 

In this Final Project Report, we present the results from final experiment for ranking predictions, in which we center on Naive Bayes as baseline method and use Bag of Words, Bi-gram, Uni-Bi-gram, and Stop Words as features to evaluate the performance of the experiment. Following the results for ranking prediction, we show the results for aspect extraction and explain the methods that have been implemented to improve aspect extraction. We also address five major errors, which we considered to have higher possibilities in affecting the accuracies in ranking predictions. We conclude the final report with some of the challenges in final experiment, especially the challenge for us to finish the test on Sentiment Analysis for each aspect, some preliminary ideas for future work, and a summary for what we have learned from this project. 
 
\section{Definitions}
\subsection{Basic Definitions}
\begin{itemize}
\item Sentiment Analysis: are computational studies of opinions, sentiments, subjectivity, evaluations, attitudes, appraisal, affects, views, and emotions.
\item Sentiment Classification: are deployed both on the document level and the sentence level. As indicated by the term classification, it is basically a text classification problem that classifies a whole opinion document/sentence based on the overall sentiment of the opinion holder.
\item Aspect-based Sentiment Analysis: are refereed to determining the opinions and sentiments expressed on different features or aspects of entities.
\end{itemize}


\subsection{Data Set Description}
We grab the data set from Stanford SNAP group and it is a data set of Amazon movie reviews crawled from the
web. For each instance, it contains the information as shown in Table~\ref{data}.  The data span a period 
of more than 10 years, including all ~8 million reviews up to October 2012. Reviews include product and 
user information, ratings, and a plain text review. We also have reviews from all other Amazon categories. 
There are in total 7911684 instances in this data set. The reviews are collected from 253059 users for 253059 
movies and among these reviewers, 16341 of them have reviews more than 50 pieces. The reviews' quality are 
pretty high as the median number of words per review is 101 \cite{mcauley2013amateurs}.

\begin{table*}[!htbp]
\centering
\caption{Dataset}\label{data}
\begin{tabular}{ll}
\hline
\hline
Name&Example\\
 \hline
product/productId& B00006HAXW\\
review/userId& A1RSDE90N6RSZF\\
review/profileName& Joseph M. Kotow\\
review/helpfulness& 9/9\\
review/score& 5.0\\
review/time& 1042502400\\
review/summary& Pittsburgh - Home of the OLDIES\\
review/text& I have all of the doo ...\\
\hline
\hline
\end{tabular}
\end{table*}


\section{Research Questions}
After observing the dataset together with some basic literature review, in this project, we are more interested in making predictions and analyzing the common features based on this dataset. We have the following  well-defined candidate question:
\begin{itemize}
\item How to generate aspect-based summaries for each movie.
% \item What is the evolution	of user	tastes and opinions (time series)?
% \item What is the taste of certain users on movie genres?
\end{itemize}
%In order to make the question more specific and the research method more feasible, we still need further discussions as well as stepping stone tests.



\section{Related Works}
Sentiment Analysis and Opinion Mining are computational studies of opinions, sentiments, subjectivity, evaluations, attitudes, appraisal, affects, views, emotions, etc., expressed in text~\cite{pang2008opinion}. Since the sentiment analysis has been a hot topic over years, there are a number of publications in this research area. Meanwhile, a number of opinion mining applications in the market, such as the opinion observe~\cite{liu2005opinion} that conducts analysis on the cellphone reviews; Aspect-based opinion summary for both the Bing search engine and Google product search~\cite{blair2008building}; tools like OpinionEQ\footnote{http://www.opinioneq.com.} that integrates a few sentiment analysis functions; and live track of movies that predicts user ratings from the Twitter posts~\cite{thet2010aspect}.  In this report of literature review, we report the related works in two perspectives: (1) sentiment classification, and (2) aspect-based sentiment analysis.

Before stepping into the first branch, we formalize the definitions of some related terms. \emph{Opinions} are those words expressed one's feeling to an object. In one piece of opinion, there are opinion targets, features, sentimental positive or negative, opinion holder as well as the time~\cite{dale2000handbook}. For example, in the sentence "Alex bought a Cannon camera two weeks ago, and he loves it because the pictures are beautiful and high quality." 
\begin{itemize}
\item (Target: Cannon camera; Features: picture quality; Sentimental pos or nag: positive"love"; Opinion holder: Alex; Time: two weeks ago)
\end{itemize}
Usually, we use the quinttuples to describe an opinion to make the unstructured data into structured data~\cite{liu2007web}. 

\subsection{Sentiment Classification}
Sentiment classifications are deployed both on the document level and the sentence level. As indicated by the term classification, it is basically a text classification problem that classifies a whole opinion document/sentence, based on the overall sentiment of the opinion holder~\cite{turney2002thumbs, pang2002thumbs}. Obviously, for a classification problem, both the \textbf{unsupervised} and \textbf{supervised} learning are adopted. 
\begin{itemize}
\item Unsupervised: Unsupervised methods derive a sentiment metric for text without training corpus, and it has been widely used, since the early time when this topic was first introduced. It is a fascinating problem for researchers to study; however, the sentiment classification is hard to deploy in the real research and experiment as there are many potential challenges in this method. Turney~\cite{turney2002thumbs} predicates the sentiment orientation of
a review by the average semantic orientation of the
phrases in the review that contain adjectives or
adverbs, which is denoted as the semantic oriented
method. They use three steps in this unsupervised classification: POS tags, Sentiment orientation(SO) estimation of the extracted phrases, and Average SO computing. Kim and Hovy~\cite{kim2004determining} build three models
to assign a sentiment category to a given sentence
by combining the individual sentiments of sentiment-
bearing words. Hiroshi~\cite{hiroshi2004deeper} use the technique of deep language analysis for machine
translation to extract sentiment units in text documents. Devitt and Ahmad~\cite{devitt2007sentiment} explore a
computable metric of positive or negative polarity
in financial news text.
\item Supervised: Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. In majority, three classification techniques are tried: Naive Bayes, Maximum entropy, and Support vector machine. A few features cater to the researchers are \emph{term frequency}, \emph{POS tag}, \emph{opinion words and phrases}, \emph{negations}, \emph{syntactic dependency}, etc. Since the work of Pang et al~\cite{pang2002thumbs}, various classification models
and linguistic features have been proposed to improve
the classification performance. Mullen and Collier~\cite{mullen2004sentiment}; Wilson et al.~\cite{wilson2005recognizing}. Most recently, McDonald et al.~\cite{titov2008joint} investigate a structured model for jointly
classifying the sentiment of text at varying levels
of granularity. Blitzer et al.~\cite{blitzer2007biographies} investigate domain
adaptation for sentiment classifiers, focusing
on on line reviews for different types of products.
Andreevskaia and Bergler~\cite{andreevskaia2007clac} present a new
system consisting of the ensemble of a corpus based
classifier and a lexicon-based classifier with
precision-based vote weighting.

This Sentiment Classification method has been well-studied and implemented in many research projects; however, the potential challenges to deploy such method are also obvious. After reviewing related literatures in this topic, the Sentiment Classification method has the following limitations:
\begin{itemize}
\item The Sentiment Classification work for only one object in the document or sentence
\item This method cannot extract different opinions
\item This method cannot correctly extract indirect/unobvious opinions
\item this method does not work for comparison reviews
\end{itemize}
For example, in the sentence ``We bought the car last month and the windshield wiper has fallen off.'' There are two targets mentioned in this sentence, car and windshield wiper, and the opinion identification is unobvious. The Sentiment Classification method cannot detect whehter an opinion towards the car or the windshield wiper in this sentence.
\end{itemize}

\subsection{Aspect-based Sentiment Analysis}
Sentiment classification method at both the document and sentence levels is quite useful; however, it does not find out what people like or dislike. In this case, another branch of Sentiment Analysis called Aspect-based Sentiment Analysis emerges. This method extracts entities and aspects (target, feature, opinion, and time) from documents.

To extract the entities, some methods are considered by researchers: Distributional similarity~\cite{jo2011aspect}(which compares the surrounding text of candidates using cosine or PMI), PU learning~\cite{liu2002partially}(which learns from positive and unlabeled examples), and bayesian sets~\cite{heller2005bayesian}.

To extract the aspects, Liu first introduces a frequency-based method in 2004~\cite{hu2004mining} because he considers the reviews from different people are irrelevant. When aspects/features are discussed, the words used converge. Later on, various improved methods are applied based on the first one. Zhuang et al~\cite{zhuang2006movie} improve the recall due to loss of infrequent aspects by using opinion words to extract the aspects; Popescu and Etzioni~\cite{popescu2005opine} improve the precision by removing the frequent noun phrases that may not be aspects using part-of its relationship; Qiu~\cite{qiu2011opinion} applies the double propagation (DP) approach, which uses dependency of opinions and aspects to extract both aspects and opinion words. 


% \subsection{Appications}
% To make use of the reviews, researchers have been focusing on several applications, including extracting the summarizations either in single paragraph~\cite{beineke2004exploring} or in structured sentence list~\cite{hu2004mining}; classification for subjective and objective descriptions~\cite{wiebe2005creating}; and analysing the sentiment in document level~\cite{mullen2004sentiment}, sentence level~\cite{yu2003towards} or phrase level~\cite{zhuang2006movie}. In addition, the features learned from reviews can be utilized to developing the recommendation systems~\cite{jakob2009beyond}.


% \subsection{Methods}
% Both supervised and unsupervised learning have been adopted to mine the reviews. We will report more in this section with more literature reviews.

\subsection{Discussion}
According to the data set that we obtain, the meta-data of the data set is well-structured and intuitive so that we know the name of review commenter, movie name, time of the review, and the review itsefl from the database. After considering the advantages and disadvantages of the two sentiment analysis methods, we believe that the Aspect-based Sentiment Analysis method is the most suitable method is deploy in our research topic. This method can help us extracting different opinions in the review as reviewers usually have different opinions for different parts of a movie. In addition, this method can help us identifying comparison opinions and unobvious/indirect opinions in the review. Although the Aspect-based Sentiment Analysis method seems to match the database that we use in the following research, there are several potential problems that we need to consider:
\begin{itemize}
\item Identify different comparative and implicit opinions
\item Identify reviewer's emotions
\item Measurement of the level of opinions that matches related ratings
\end{itemize}

\section{Research Methods}
In order to answer the research questions, there are three steps in Aspect-based Sentiment Analysis: 
\begin{itemize}
\item Ranking Prediction: Given the review, we predict the ranking which ranges from 1-star to 5-star. We also predict positive when ranking is from 4-star to 5-star and negative when ranking is from 1-star to 3-star;
\item Aspect Extraction: Given the review, we extract all the possible aspect discussed.
\end{itemize}

\subsection{Ranking Prediction}
We use Naive Bayes for movie view ranking prediction. Since the ranking ranges from 1-star to 5 star, it is a Multinomial (5-class) classification. In addition, we also train a model to predict the positiveness of each review which renders a review positive when the ranking is 4-star or 5 star and negative in the other cases. Basically, it is a supervised learning model and Figure~\ref{bayes} illustrates the whole process.  In our baseline method, we adopt the bag of words as the feature. In order to obtain different results for performance evaluation and receive the most accurate prediction, we also try another four different features to fit the model. The features are:
\begin{itemize}
\item Bi-gram
\item Bag of Words and Bi-gram
\item Bag of Words with Stop Words Out
\item Bag of Words on JJ, JJS, VB (Stop Words Out)
\end{itemize}

\begin{figure}[!tp]
\centering
\includegraphics[width=0.7\columnwidth]{bayes.png}
\caption{Supervised Learning Process}
\label{bayes}
\end{figure}


\subsection{Aspect Extraction}
According to literature reviews about this research topic, we tried three major methods for Aspect Extraction: Frequent, PMI based, and Double Propagation. 
% After considering the advantages and disadvantages of all three methods for our research project on movie reviews, which contains massive words regarding aspects, entities, and opinions, we decided to use Double Propagation method as our baseline method for Aspect Extraction. 
	
	Frequent method is the easiest way to apply on Aspect-based Sentiment Analysis; however, there can be so many infrequent words in movie reviews toward the movie’s casts, content, and features, it will be difficult to extract those infrequent aspects when using Frequent method. 

The PMI-based method provides better precision with smaller drop in call. The major parts of the Infrequent methods are part-of-relationship and the Web with PMI score. The mathematical expression of the Infrequent is given as PMI(word1, word2) = log((p(word1&word2)/[(p(word1)*p(word2)]). However, the Infrequent method is not ideal when considering movie reviews, since it is generally domain dependent,  where a lot of times the aspects will approximate with the nearest noun to the opinion word.

The Double Propagation method, which is a bootstrapping method,  could be the ideal method for us to mining the movie reviews, since this method uses dependency of opinions and  aspects to extract both opinion and aspect words with domain independent method.  It inputs a set of seed opinion words, while it doesn't need aspect seeds.

We try all the three methods and make a simple comparison in the next section.

% \subsection{Identify Aspect Synonym by WordNet}
% We use the WordNet method for this step. Generally, the WordNet  sees every two synonyms as the same term, which means word A and word B will be regarded as synonyms only if there is a synset containing A and B that appear in the top two senses of both words. There are some clustering features in the WordNet, including lexical similarity, distributional information, syntactical constrains. 

% \subsection{Aspect Sentiment Classification by Naive Bayes}
% We use Naive Bayes for Aspect Sentiment Extraction. There are three steps in sentence classification: Part-of-speech (POS) tagging, Sentiment Orientation of extracted phrases, and computation of the average SO of all phrases. The mathematical expression for the second step is: 
% \begin{itemize}
% \item SO(phrase) = PMI(phrase, “excellent”) - PMI(phrase, “poor”)
% \end{itemize}
% The bigger SO means that the opinion of the aspect is positive. After computing the average SO for all phrase, the method returns the overall SO, which means the overall opinion for a certain aspect is positive or negative.  
% PMI(“top-notch plot”, “excellent”) =  2.780
% PMI(“top-notch plot”, “poor”) =  0.110
% 		SO(“top-notch plot”) = 2.670
% PMI(“really unforgetble”, “excellent”) = 0.629
% PMI(“really unforgetble”, “excellent”) = 0.237
% 		SO(“really unforgetble”)=0.392
% mean(SO)=(2.670+0.392)/2=3.062 > threshold => positive

\section{Experiment}

\subsection{Pre-processing}
To analyze the messy data, we are supposed to make it clean first. To pre-process the data, we conduct the following steps:
\begin{itemize}
\item Clean the pieces of reviews with incomplete data (such as no score or no movie name);
\item Sampling from the whole dataset with random choices. We get out 11040 pieces of reviews.
\item Transform the .txt file to .csv file in order to separate the data by the score(1.0-5.0);
\item Separate the reviews into [1-5].txt files that each of them contains the reviews of a typical score;
\item Merge [1-3].txt into n.txt; Merge [4-5].txt into y.txt;
\item Pre-process the text with stemming.
\end{itemize}

After the preprocessing, we get the data cleaned and separated in five .txt files for five rankings (1-star to 5-star) and each line in the .txt file is a review with the corresponding ranking star.


\subsection{Ranking Prediction}
We conducted 10-fold cross validation for the Naive Bayes Model. To evaluate the fitted model, we demonstrate the results in terms of \emph{Accuracy}, \emph{Precision}, \emph{Recall}, and \emph{F-score}. We also analyze the 10 most important features in each classifier we trained.

\subsubsection{Numeric Results}
Table~\ref{result1} shows the 5-class classification results in each model and Table~\ref{result2} shows the 2-class classification results.

\begin{table*}[!htbp]
\centering
\caption{Results for 5-class Prediction}\label{result1}
\begin{tabular}{lcccc}
\hline
\hline
Measurement& BoW\footnote{Bag of Words} & Bi-Gram & BoW \& Bi-Gram & Non-SW\footnote{No Stop Words}\\
 \hline
Accuracy&31.66\%&30.96\%&30.56\%&37.35\%\\
Precision&29.06\%&20.06\%&24.07\%&29.19\%\\
Recall&29.24\%&39.12\%&33.96\%&48.23\%\\
F-Score&29.16\%&29.09\%&28.17\%&36.37\%\\
\hline
\hline
\end{tabular}
\end{table*}

\begin{table*}[!htbp]
\centering
\caption{Results for 2-class Prediction}\label{result2}
\begin{tabular}{lcccccc}
\hline
\hline
Measurement& BoW & Bi-Gram & BoW \& Bi-Gram & Non-SW & Balance\footnote{Data set balanced}& POS\\
 \hline
Accuracy&69.99\%&71.55\%&67.22\%&73.01\%&75.53\%&73.55\%\\
Precision&68.65\%&69.50\%&64.26\%&70.11\%&71.74\%&70.02\%\\
Recall&68.71\%&69.92\%&65.79\%&72.16\%&74.39\%&72.04\%\\
F-Score&69.18\%&69.71\%&65.02\%&71.12\%&73.04\%&71.36\%\\
\hline
\hline
\end{tabular}
\end{table*}

\subsubsection{Feature Study}
We conduct a further inspection on the most important features shown in Table~\ref{feature} and together with the numeric results, we can get more interesting observations:
\begin{itemize}
\item To backtrack the reason that the word ``save'' come to the most important feature, we look into the reviews and find out that words like ``save your time'' and ``save your money'' appear quite often in the rank-1 reviews;
\item The adoption of Bi-Gram introduced much more representative features into our model which did not show their power in the Bag of Words features, such as ``2 star'' and ``absolutely no'';
\item There are some repetition in the mixed features such as ``save'' and ``save your'' and it can be the reason that cause the drop in the accuracy when we adopted both ``Bag of Words'' and ``Bi-Gram'' as the feature.
\end{itemize}
 

\begin{table*}[!htbp]
\centering
\caption{Feature Inspection}\label{feature}
\begin{tabular}{lcccc}
\hline
\hline
Model& Most Important Features\\
 \hline
BoW (5-class)&Save&Josh&Bush\\
&stupid&Damon&toilet\\
&awful&angel&jump&\\
BoW (2-class)&horrible&stupid&Save\\
&crap& shoddy&McLintock\\
&zero&Sorry&garbage\\
Bi-Gram (5-class)&2 star&Save your&3 star\\
&film are&come off&it two\\
&it two&as bad&waste of\\
Bi-Gram (2-class)&Save your&Don't waste&3 star\\
&excuse for&horror movie&it two\\
&bad movie&have high&absolutely no\\
BoW\&Bi-Gram (2-class)&2 star&Save&Josh\\
&save your&3 star&file are\\
&come off&it two&the machine\\
\hline
\hline
\end{tabular}
\end{table*}

\subsubsection{Case Study}
After the statistical calculation which shows the overall performance of the Naive Bayes model, we also backtrack the mis-classified cases and in summary, there are 4 major error types:
\begin{itemize}
\item "Not" cases: The negative words appear in the form of ``Not''+ positive words. In this case, with such positive words as the features, the classifier will easily treat it as a review with positive sentiment. e.g.,
I know that it is cliche to say that the movie is not as good as the book. (1'-4')
\item Feeling changes: The review contains some feeling changes that both positive and negative words exist at the same time. There is no order in the features we use in our system, in which case, mis-classification error may happen. e.g.,
At first I was so excited to order this because I have heard so much about Tina Landon was eager to give this a try.  I found it so difficult to keep up with Tina you basically need to have some sort of training in choreography to understand how to do some of the moves. (1'-4')
\item Different Things: The review is supposed to talk about the movie itself yet sometimes it refers to some other stuffs, such as the adapted book or the director's other movie. It can be quite misleading for our system to recognize the real feel of the reviewer towards the movie. e.g., I had just finished reading the book and decided I'd like see how it was adapted to the big screen.  This version was the worst adaptation I've seen of a book made into a movie.  As I watched the film I wondered if the makers of the movie even bothered to read the book.  My girlfriend kept asking me during the film if ``that happened in the book?''  At the end of the movie I told her that basically everything in the movie was not how it occurred in the book.  She enjoyed the movie until the end when it was like a ``cheesy 80's montage''.  The book is wonderful.  When I finished reading it, I felt like I had lost a good friend-I wished I could just keep reading it and reading it.  I've never read such an excellent novel. (1'-5')
\item No significant evidence: The sentiment in the review is supposed to match the ranking, however, it is not always what happens. Sometimes, the reviewer rank the movie with extreme high or low score but put so little evidence in the review that reveals his true feeling. Thus, the prediction from the classifier may tender to give a relatively mild score. e.g., I recvd this video (DVD version) as a Christmas gift.  I put it on about 11 pm just to see what it was like, and finally got dragged to bed about 4 (5'-3')
\end{itemize}

\subsubsection{Comparison}
Reviewing the results we get from the model and considering the adjustments in each round of the experiment, we can draw the following conclusions:
\begin{enumerate}
\item Make the dataset balanced will make a great contribution to the accuracy and F-Score.
\item Bi-Gram improves the accuracy as it solves the 'No' case problem to some extent and involves many more valuable features.
\item Bag-of-Words + Bi-Gram might make the model over fit.
\item No Stop Words can perform better.
\end{enumerate}

\subsection{Aspect Extraction}
To extract the aspect in the review, three methods from previous studies are tried: Frequency-Based, PMI-Based and Double Propagation. We extracted 71, 163, and 107 aspects respectively from the review sampling by using the three methods. And we find that the Frequency-based method cannot extract all the aspect and PMI-based method involves many irrelevant nouns into the aspect set which is both not accurate and time-consuming. In this case, Double Propagation performs the best among the three. Taking advantage of Double Propagation, we extract all the aspects from the review and visualize it in the Word Cloud which is shown in Figure~\ref{wordcloud}. We keep the frequency of each aspect to generate the Word Cloud: The bigger the font is, the more frequent the aspect was talked about in the reviews. 

\begin{figure}[!tp]
\centering
\includegraphics[width=0.7\columnwidth]{a.png}
\caption{Aspect Word Cloud}
\label{wordcloud}
\end{figure}

\section{Limitation}
We only adopted a very basic data mining model, Naive Bayes, to treat this classification problem. The models fitted both in the 5-class and 2-class classifications do not achieve a relatively high performance in terms of accuracy and F-score. To better predict the ranking, we should further involve models which could handle the features in an ordered way in spite of the current ones. To really ``understand'' the semantics of the reviews but not treat them word by word. In addition to improve the prediction accuracy, in the future, more works can be conducted on the sentiment for each aspect in the review which is more interesting and useful.

\section{Contribution}
Xiao contributed on coding and documentation. Wenxiang contributed to a part of the writeup of this paper, and he also contributed on proofreading and editing the paper.
\medskip

\printbibliography[title={Citations}]

\printbibliography[type=article,title={Articles only}]
\printbibliography[type=book,title={Books only}]

\printbibliography[keyword={physics},title={Physics-related only}]
\printbibliography[keyword={latex},title={\LaTeX-related only}]
\printbibliography[keyword={knuth},title={Knuth's-related only}]

\end{document}
